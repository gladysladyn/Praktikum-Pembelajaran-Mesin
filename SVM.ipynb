{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing various libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import _pickle as cPickle\n",
    "from scipy.io import loadmat\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('white')\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer ,TfidfVectorizer,TfidfTransformer\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV, learning_curve\n",
    "from textblob import download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "liked",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "ae555354-8706-4a09-93f9-6a7d3d1d8f95",
       "rows": [
        [
         "0",
         "1",
         "India is developing countries"
        ],
        [
         "1",
         "1",
         "The Da Vinci Code book is just awesome."
        ],
        [
         "2",
         "1",
         "this was the first clive cussler i've ever read, but even books like Relic, and Da Vinci code were more plausible than this."
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>liked</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>India is developing countries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The Da Vinci Code book is just awesome.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>this was the first clive cussler i've ever rea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   liked                                               text\n",
       "0      1                      India is developing countries\n",
       "1      1            The Da Vinci Code book is just awesome.\n",
       "2      1  this was the first clive cussler i've ever rea..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"Training.txt\",sep=\"\\t\", names=['liked','text'],encoding=\"utf-8\");\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is downloaded from https://www.kaggle.com/c/si650winter11/data\n",
    "this is a TSV (\"tab separated values\") file, where the first column is a label saying whether the given review\n",
    "is positive or negative. The second column is the review itself.\n",
    "Data is tab separeted and therefore \"\\t\" is passed as separator parameter to function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6931\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total no of reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "liked",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "('text', 'count')",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "('text', 'unique')",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "('text', 'top')",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "('text', 'freq')",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "85242b49-a072-4b32-b9f3-e49d7f3def01",
       "rows": [
        [
         "0",
         "2975",
         "559",
         "I hate Harry Potter.",
         "85"
        ],
        [
         "1",
         "3956",
         "744",
         "I love Harry Potter.",
         "167"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liked</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2975</td>\n",
       "      <td>559</td>\n",
       "      <td>I hate Harry Potter.</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3956</td>\n",
       "      <td>744</td>\n",
       "      <td>I love Harry Potter.</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text                                  \n",
       "      count unique                   top freq\n",
       "liked                                        \n",
       "0      2975    559  I hate Harry Potter.   85\n",
       "1      3956    744  I love Harry Potter.  167"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('liked').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(review):\n",
    "    return TextBlob(review).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   [India, is, developing, countries]\n",
       "1      [The, Da, Vinci, Code, book, is, just, awesome]\n",
       "2    [this, was, the, first, clive, cussler, i, 've...\n",
       "3             [i, liked, the, Da, Vinci, Code, a, lot]\n",
       "4             [i, liked, the, Da, Vinci, Code, a, lot]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head().text.apply(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function tokens() is created to parse data/review into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ready', 'NN'),\n",
       " ('was', 'VBD'),\n",
       " ('not', 'RB'),\n",
       " ('a', 'DT'),\n",
       " ('good', 'JJ'),\n",
       " ('movie', 'NN')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"ready was not a good movie\").tags\n",
    "#nltk.help.upenn_tagset('JJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".tags is inbuilt function provided by TextBlob it is used to assign part of speech tags to the words in text.\n",
    "It gives list of (word, POS) pairs.\n",
    "To check meaning of particular tag nltk.help.upenn_tagset('tagname') can be used eg. nltk.help.upenn_tagset('JJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     [india, is, developing, country]\n",
       "1      [the, da, vinci, code, book, is, just, awesome]\n",
       "2    [this, wa, the, first, clive, cussler, i, 've,...\n",
       "3             [i, liked, the, da, vinci, code, a, lot]\n",
       "4             [i, liked, the, da, vinci, code, a, lot]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_lemmas(review):\n",
    "    wordss = TextBlob(review.lower()).words\n",
    "    # for each word, take its \"base form\" = lemma \n",
    "    return [word.lemma for word in wordss]\n",
    "\n",
    "df.text.head().apply(to_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is one of the important satge of data preprocessing in this step words are converted to their lemma(base form). For example \"octopi\" is converted to \"octopus\". similar method is stemming.\n",
    "\n",
    "NLTK also provide very powerful lemmatizer which make use of WORDNET eg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'octopus'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "lmtzr.lemmatize('octopi')\n",
    "#nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting text data into vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2114\n"
     ]
    }
   ],
   "source": [
    "bow_transformer = CountVectorizer(analyzer=to_lemmas).fit(df['text'])\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn offers Countvectorizer which counts the frequency of particular word in document. This assigns a unique number to every word in collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i liked the Da Vinci Code a lot.\n"
     ]
    }
   ],
   "source": [
    "review1=df['text'][3]\n",
    "print(review1)\n",
    "#to check 3rd document/review in collection/database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 8 stored elements and shape (1, 2114)>\n",
      "  Coords\tValues\n",
      "  (0, 42)\t1\n",
      "  (0, 369)\t1\n",
      "  (0, 458)\t1\n",
      "  (0, 950)\t1\n",
      "  (0, 1123)\t1\n",
      "  (0, 1152)\t1\n",
      "  (0, 1838)\t1\n",
      "  (0, 1977)\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 2114)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow=bow_transformer.transform([review1])\n",
    "print(bow)\n",
    "bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countvectorizer creates sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code-other\n"
     ]
    }
   ],
   "source": [
    "print(bow_transformer.get_feature_names_out()[372])\n",
    "#to check 372nd word in collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse matrix shape: (6931, 2114)\n",
      "number of non-zeros: 71297\n",
      "sparsity: 7129700.00%\n"
     ]
    }
   ],
   "source": [
    "review_bow = bow_transformer.transform(df['text'])\n",
    "print( 'sparse matrix shape:', review_bow.shape)\n",
    "print('number of non-zeros:', review_bow.nnz) #learn this\n",
    "print( 'sparsity: %.2f%%' % (100.0 * review_bow.nnz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shape of sparse matrix n*m where n are total documents and m are total unique words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer just count the frequency of word in that document. But many time few words such as(the, or) occurs a lot of time in collection which really don't contribute in deciding the polarity of particular document so to nullify their effect special weighting method is to be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf means term-frequency while tf-idf means term-frequency times inverse document-frequency.\n",
    "\n",
    "The goal of using tf-idf instead of the just CountVectorizer in a given document is to scale down the impact of tokens that occur very frequently in a given corpus(which are less informative) than tokens which occur few times.\n",
    "\n",
    "Tf-idf(d,t)=tf(d,t)*idf(d,t);\n",
    "\n",
    "where tf(d,t) is term frequency which states how many times word/token t occur in that doucument devided by total no of words in that document.\n",
    "\n",
    "and idf(d,t)=log[n/(df(d,t))] i.e. total no of documents divided by no of documents containing that word/token t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6931, 2114)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer =TfidfTransformer().fit(review_bow)\n",
    "review_tfidf = tfidf_transformer.transform(review_bow)\n",
    "review_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count vectorizer gives output as frequency of diiferent words in our corpus this is then passed to transform method of tf-idf_transformer.\n",
    "\n",
    "This Transform a count matrix to a normalized tf or tf-idf representation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5544 1387 5544 1387\n"
     ]
    }
   ],
   "source": [
    "text_train, text_test, liked_train, liked_test = train_test_split(df['text'], df['liked'], test_size=0.2)\n",
    "print(len(text_train), len(text_test), len(text_train) , len(text_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset downloaded is then divided into training data and test data with ratio 0.8 to 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_svm = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=to_lemmas)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('classifier', SVC()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The final estimator only needs to implement fit.\n",
    "\n",
    "The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline parameters to automatically explore and tune\n",
    "param_svm = [\n",
    "  {'classifier__C': [1, 10, 100, 1000], 'classifier__kernel': ['linear']},\n",
    "  {'classifier__C': [1, 10, 100, 1000], 'classifier__gamma': [0.001, 0.0001], 'classifier__kernel': ['rbf']},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_svm = GridSearchCV(\n",
    "    pipeline_svm, #object used to fit the data\n",
    "    param_grid=param_svm, \n",
    "    refit=True,  # fit using all data, on the best detected classifier\n",
    "    n_jobs=-1,  # number of cores to use for parallelization; -1 for \"all cores\" i.e. to run on all CPUs\n",
    "    scoring='accuracy',#optimizing parameter\n",
    "    cv=StratifiedKFold(n_splits=5),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exhaustive search over specified parameter values for an estimator.\n",
    "\n",
    "CV stands for cross validations. Learning the parameters of a prediction function and testing it on the same data is a methodological mistake, it will always give 100% accuracy and therefore training and testing data must be different. Cross validation is idea of dividing training data into k folds i.e. k subset. The following procedure is followed for each of the k “folds”:\n",
    "\n",
    "A model is trained using k-1 of the folds as training data;\n",
    "\n",
    "The resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.23 s\n",
      "Wall time: 40.7 s\n",
      "{'mean_fit_time': array([2.72254987, 2.62624483, 2.34099193, 2.33624911, 7.3307641 ,\n",
      "       7.05083861, 5.18240552, 6.61948762, 2.8871954 , 4.96339154,\n",
      "       2.57925725, 2.61083131]), 'std_fit_time': array([0.20636389, 0.34558952, 0.08352396, 0.0803921 , 0.34818805,\n",
      "       0.34274551, 0.12838321, 0.10007778, 0.0838884 , 0.07088953,\n",
      "       0.09419189, 0.19244684]), 'mean_score_time': array([0.52959766, 0.53667607, 0.52941546, 0.57351928, 1.45371943,\n",
      "       1.43254509, 1.11995158, 1.40099459, 0.65607615, 1.11905184,\n",
      "       0.59287043, 0.56156497]), 'std_score_time': array([0.02622732, 0.025362  , 0.01728358, 0.03841281, 0.03411989,\n",
      "       0.06142917, 0.07135744, 0.04334649, 0.02534097, 0.03605269,\n",
      "       0.03674763, 0.0807875 ]), 'param_classifier__C': masked_array(data=[1, 10, 100, 1000, 1, 1, 10, 10, 100, 100, 1000, 1000],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value=999999), 'param_classifier__kernel': masked_array(data=['linear', 'linear', 'linear', 'linear', 'rbf', 'rbf',\n",
      "                   'rbf', 'rbf', 'rbf', 'rbf', 'rbf', 'rbf'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value=np.str_('?'),\n",
      "            dtype=object), 'param_classifier__gamma': masked_array(data=[--, --, --, --, 0.001, 0.0001, 0.001, 0.0001, 0.001,\n",
      "                   0.0001, 0.001, 0.0001],\n",
      "             mask=[ True,  True,  True,  True, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value=1e+20), 'params': [{'classifier__C': 1, 'classifier__kernel': 'linear'}, {'classifier__C': 10, 'classifier__kernel': 'linear'}, {'classifier__C': 100, 'classifier__kernel': 'linear'}, {'classifier__C': 1000, 'classifier__kernel': 'linear'}, {'classifier__C': 1, 'classifier__gamma': 0.001, 'classifier__kernel': 'rbf'}, {'classifier__C': 1, 'classifier__gamma': 0.0001, 'classifier__kernel': 'rbf'}, {'classifier__C': 10, 'classifier__gamma': 0.001, 'classifier__kernel': 'rbf'}, {'classifier__C': 10, 'classifier__gamma': 0.0001, 'classifier__kernel': 'rbf'}, {'classifier__C': 100, 'classifier__gamma': 0.001, 'classifier__kernel': 'rbf'}, {'classifier__C': 100, 'classifier__gamma': 0.0001, 'classifier__kernel': 'rbf'}, {'classifier__C': 1000, 'classifier__gamma': 0.001, 'classifier__kernel': 'rbf'}, {'classifier__C': 1000, 'classifier__gamma': 0.0001, 'classifier__kernel': 'rbf'}], 'split0_test_score': array([0.99008115, 0.98917944, 0.98917944, 0.98917944, 0.5716862 ,\n",
      "       0.5716862 , 0.96663661, 0.5716862 , 0.9864743 , 0.96663661,\n",
      "       0.99008115, 0.9864743 ]), 'split1_test_score': array([0.99098287, 0.99098287, 0.99098287, 0.99098287, 0.5716862 ,\n",
      "       0.5716862 , 0.97565374, 0.5716862 , 0.98827773, 0.97565374,\n",
      "       0.99098287, 0.98827773]), 'split2_test_score': array([0.99549143, 0.99278629, 0.99278629, 0.99278629, 0.57078449,\n",
      "       0.57078449, 0.97114518, 0.57078449, 0.99188458, 0.97114518,\n",
      "       0.99458972, 0.99188458]), 'split3_test_score': array([0.99188458, 0.99098287, 0.99098287, 0.99098287, 0.57078449,\n",
      "       0.57078449, 0.97024346, 0.57078449, 0.98917944, 0.97024346,\n",
      "       0.99008115, 0.98917944]), 'split4_test_score': array([0.99368231, 0.99458484, 0.99458484, 0.99458484, 0.57129964,\n",
      "       0.57129964, 0.97472924, 0.57129964, 0.98736462, 0.97472924,\n",
      "       0.99458484, 0.98736462]), 'mean_test_score': array([0.99242447, 0.99170326, 0.99170326, 0.99170326, 0.57124821,\n",
      "       0.57124821, 0.97168165, 0.57124821, 0.98863613, 0.97168165,\n",
      "       0.99206395, 0.98863613]), 'std_test_score': array([0.00194161, 0.00183761, 0.00183761, 0.00183761, 0.00040408,\n",
      "       0.00040408, 0.00325189, 0.00040408, 0.0018583 , 0.00325189,\n",
      "       0.00208644, 0.0018583 ]), 'rank_test_score': array([ 1,  3,  3,  3, 10, 10,  8, 10,  6,  8,  2,  6], dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "%time classifier = grid_svm.fit(text_train, liked_train) # find the best combination from param_svm\n",
    "print(classifier.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       598\n",
      "           1       0.99      1.00      0.99       789\n",
      "\n",
      "    accuracy                           0.99      1387\n",
      "   macro avg       0.99      0.99      0.99      1387\n",
      "weighted avg       0.99      0.99      0.99      1387\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(liked_test, classifier.predict(text_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(classifier.predict([\"the vinci code is awesome\"])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(classifier.predict([\"the vinci code is bad\"])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.32465246735834974)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gaussKernel(x1, x2, sigma):\n",
    "    ss=np.power(sigma,2)\n",
    "    norm= (x1-x2).T.dot(x1-x2)\n",
    "    return np.exp(-norm/(2*ss))\n",
    "x1 = np.array([1, 2, 1])\n",
    "x2 = np.array([0, 4, -1])\n",
    "sigma = 2\n",
    "gaussKernel(x1,x2,sigma)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sentiment-analysis-using-SVM-master-Fly90WZh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
